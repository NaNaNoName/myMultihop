{
 "cells": [
  {
   "cell_type": "code",
   "id": "31f7b70fb5993f0b",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-16T14:23:37.715224Z",
     "start_time": "2025-04-16T14:23:35.212243Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "import json\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"\n",
    "    加载JSON文件数据\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def split_by_source_random(data, num_groups):\n",
    "    \"\"\"\n",
    "    根据来源(source)将数据随机分成num_groups份    \n",
    "    Args:\n",
    "        data: JSON数据列表\n",
    "        num_groups: 需要分成的组数\n",
    "    Returns:\n",
    "        分组后的数据字典，键为组号，值为该组的数据列表\n",
    "    \"\"\"\n",
    "    # 按source分组\n",
    "    source_groups = defaultdict(list)\n",
    "    for item in data:\n",
    "        source = item.get('source', 'Unknown')\n",
    "        source_groups[source].append(item)\n",
    "    \n",
    "    # 创建空组\n",
    "    result_groups = {i: [] for i in range(num_groups)}\n",
    "    \n",
    "    # 获取所有来源并随机排序\n",
    "    sources = list(source_groups.keys())\n",
    "    random.shuffle(sources)\n",
    "    \n",
    "    # 为每个来源分配一个组\n",
    "    for i, source in enumerate(sources):\n",
    "        group_idx = i % num_groups  # 循环分配来源到各组\n",
    "        result_groups[group_idx].extend(source_groups[source])\n",
    "    \n",
    "    return result_groups\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def split_by_source_balanced(data, num_groups):\n",
    "    \"\"\"\n",
    "    根据来源(source)将数据均衡地分配到num_groups个组中\n",
    "    \n",
    "    Args:\n",
    "        data: JSON数据列表，每个元素需包含'source'字段\n",
    "        num_groups: 需要分成的组数\n",
    "    \n",
    "    Returns:\n",
    "        dict: 键为组号(0~num_groups-1)，值为该组的数据列表\n",
    "    \"\"\"\n",
    "    # 1. 按source分组并统计每个来源的数据量\n",
    "    source_groups = defaultdict(list)\n",
    "    source_counts = defaultdict(int)\n",
    "    for item in data:\n",
    "        source = item.get('source', 'Unknown')\n",
    "        source_groups[source].append(item)\n",
    "        source_counts[source] += 1\n",
    "\n",
    "    # 2. 按数据量从大到小排序来源（优先分配大数据量来源）\n",
    "    sorted_sources = sorted(source_counts.keys(), \n",
    "                           key=lambda x: -source_counts[x])\n",
    "\n",
    "    # 3. 初始化各组（记录当前组的数据量）\n",
    "    result_groups = {i: [] for i in range(num_groups)}\n",
    "    group_counts = [0] * num_groups  # 记录各组当前数据量\n",
    "\n",
    "    # 4. 贪心算法分配：总是将当前来源分配给数据量最少的组\n",
    "    for source in sorted_sources:\n",
    "        # 找到当前数据量最少的组\n",
    "        target_group = min(range(num_groups), key=lambda x: group_counts[x])\n",
    "        # 将整个来源的数据分配到该组\n",
    "        result_groups[target_group].extend(source_groups[source])\n",
    "        group_counts[target_group] += source_counts[source]\n",
    "\n",
    "    # 5. 打印各组数量分布（调试用）\n",
    "    print(\"各组数据量分布:\", group_counts)\n",
    "    \n",
    "    return result_groups\n",
    "\n",
    "def save_split_data(groups, output_dir):\n",
    "    \"\"\"\n",
    "    将分组数据保存到文件\n",
    "    Args:\n",
    "        groups: 分组后的数据字典\n",
    "        output_dir: 输出目录\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for group_idx, group_data in groups.items():\n",
    "        output_path = os.path.join(output_dir, f'group_{group_idx}.json')\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(group_data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"保存组 {group_idx} 到 {output_path}，包含 {len(group_data)} 条数据\")\n",
    "\n",
    "def split_by_clustering(data, num_groups):\n",
    "    \"\"\"\n",
    "    todo: 效果很差，还没找到一个聚类效果好的办法\n",
    "    使用聚类方法将数据分成num_groups份 \n",
    "    Args:\n",
    "        data: JSON数据列表\n",
    "        num_groups: 需要分成的组数\n",
    "    Returns:\n",
    "        分组后的数据字典，键为组号，值为该组的数据列表\n",
    "    \"\"\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T14:23:39.994875Z",
     "start_time": "2025-04-16T14:23:39.825708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 设置文件路径\n",
    "input_file = 'dataset/corpus.json'  # 替换为你的JSON文件路径\n",
    "\n",
    "# 加载数据\n",
    "data = load_json_data(input_file)\n",
    "print(f\"加载了 {len(data)} 条数据\")\n",
    "\n",
    "# 示例1：按source均匀分成3组\n",
    "groups_by_source = split_by_source_balanced(data, 3)\n",
    "save_split_data(groups_by_source, 'splits/balance_3')"
   ],
   "id": "cf0cfa6db73f7964",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载了 609 条数据\n",
      "各组数据量分布: [203, 203, 203]\n",
      "保存组 0 到 splits/balance_3\\group_0.json，包含 203 条数据\n",
      "保存组 1 到 splits/balance_3\\group_1.json，包含 203 条数据\n",
      "保存组 2 到 splits/balance_3\\group_2.json，包含 203 条数据\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
