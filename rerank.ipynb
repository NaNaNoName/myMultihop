{
 "cells": [
  {
   "cell_type": "code",
   "id": "e6097f0d8a47a0a1",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-17T09:27:06.202854Z",
     "start_time": "2025-04-17T09:27:06.162826Z"
    }
   },
   "source": [
    "import concurrent.futures\n",
    "\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.embeddings.dashscope import DashScopeEmbedding\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Union\n",
    "from tqdm import tqdm\n",
    "from dashscope import TextReRank\n",
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "from llama_index.core.schema import NodeWithScore, MetadataMode\n",
    "from llama_index.core import Settings\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "class UnifiedEmbedding:\n",
    "    \"\"\"统一嵌入模型接口\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str = \"dashscope\",\n",
    "        model_name: str = \"text-embedding-v2\",\n",
    "        dashscope_text_type: str = \"document\",\n",
    "        api_key: str = None,\n",
    "        base_url: str = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_type: 模型类型 (openai/dashscope)\n",
    "            model_name: 模型名称\n",
    "            dashscope_text_type: DashScope文本类型\n",
    "            api_key: API密钥\n",
    "            base_url: 服务地址\n",
    "        \"\"\"\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        if model_type == \"dashscope\":\n",
    "            self.embedder = DashScopeEmbedding(\n",
    "                model_name=model_name,\n",
    "                text_type=dashscope_text_type,\n",
    "                api_key=api_key\n",
    "            )\n",
    "        else:\n",
    "            self.embedder = OpenAIEmbedding(\n",
    "                # openai 默认的嵌入模型\n",
    "                # model: str = OpenAIEmbeddingModelType.TEXT_EMBED_ADA_002,\n",
    "                # model=model_name,\n",
    "                api_key=api_key,\n",
    "                api_base=base_url\n",
    "            )\n",
    "\n",
    "    def get_embed_model(self):\n",
    "        return self.embedder\n",
    "\n",
    "def load_indexes_from_folder(storage_dir: str) -> List:\n",
    "    \"\"\"从指定文件夹加载所有索引\"\"\"\n",
    "    storage_path = Path(storage_dir)\n",
    "    if any(storage_path.glob(\"*.json\")):\n",
    "        index_dirs = [storage_dir]\n",
    "    else:\n",
    "        index_dirs = sorted([d for d in Path(storage_dir).iterdir() if d.is_dir()])\n",
    "    return [load_index_from_storage(\n",
    "        StorageContext.from_defaults(persist_dir=str(d))\n",
    "    ) for d in index_dirs]\n",
    "\n",
    "def process_retrieval(\n",
    "    data: Dict,\n",
    "    retrievers: List,\n",
    "    use_rerank: bool = False,\n",
    "    rerank_model: str = \"gte-rerank-v2\"\n",
    ") -> Dict:\n",
    "    \"\"\"处理单个查询的检索流程，同时返回原始和重排序结果\"\"\"\n",
    "    # 执行多索引检索\n",
    "    nodes = []\n",
    "    for retriever in retrievers:\n",
    "        nodes.extend(retriever.retrieve(data[\"query\"]))\n",
    "    \n",
    "    # 原始排序结果\n",
    "    original_sorted = sorted(nodes, key=lambda x: x.score, reverse=True)\n",
    "    \n",
    "    # 初始化重排序结果（默认与原始相同）\n",
    "    reranked_sorted = original_sorted.copy()\n",
    "    \n",
    "    # 执行重排序逻辑\n",
    "    if use_rerank and len(nodes) > 0:\n",
    "        rerank_results = TextReRank.call(\n",
    "            model=rerank_model,\n",
    "            query=data[\"query\"],\n",
    "            documents=[n.get_content() for n in nodes],\n",
    "            top_n=10,\n",
    "            return_documents=False\n",
    "        )\n",
    "        reranked_sorted = [nodes[r['index']] for r in rerank_results.output['results']]\n",
    "    \n",
    "    # 构建包含两种结果的结构\n",
    "    return {\n",
    "        \"query\": data[\"query\"],\n",
    "        \"answer\": data[\"answer\"],\n",
    "        \"question_type\": data[\"question_type\"],\n",
    "        \"retrieval_original\": format_retrieval_results(original_sorted),\n",
    "        \"retrieval_reranked\": format_retrieval_results(reranked_sorted),\n",
    "        \"gold_list\": data[\"evidence_list\"]\n",
    "    }\n",
    "\n",
    "def format_retrieval_results(nodes: List[NodeWithScore]) -> List[Dict]:\n",
    "    \"\"\"格式化检索结果\"\"\"\n",
    "    return [{\n",
    "        \"text\": node.get_content(metadata_mode=MetadataMode.LLM),\n",
    "        \"score\": node.score\n",
    "    } for node in nodes]\n",
    "\n",
    "def split_and_save_results(results: List[Dict], output_dir: Union[str, Path]):\n",
    "    \"\"\"分割并保存两种检索结果\"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    original_results = []\n",
    "    reranked_results = []\n",
    "    \n",
    "    for res in results:\n",
    "        if not res:\n",
    "            continue\n",
    "            \n",
    "        base_entry = {\n",
    "            \"query\": res[\"query\"],\n",
    "            \"answer\": res[\"answer\"],\n",
    "            \"question_type\": res[\"question_type\"],\n",
    "            \"gold_list\": res[\"gold_list\"]\n",
    "        }\n",
    "        \n",
    "        original_entry = {\n",
    "            **base_entry,\n",
    "            \"retrieval_list\": res[\"retrieval_original\"]\n",
    "        }\n",
    "        reranked_entry = {\n",
    "            **base_entry,\n",
    "            \"retrieval_list\": res[\"retrieval_reranked\"]\n",
    "        }\n",
    "        \n",
    "        original_results.append(original_entry)\n",
    "        reranked_results.append(reranked_entry)\n",
    "\n",
    "    # 保存结果文件\n",
    "    def save_data(data, filename):\n",
    "        with open(output_dir / filename, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "    \n",
    "    save_data(original_results, \"retrieval_results_original.json\")\n",
    "    save_data(reranked_results, \"retrieval_results_reranked.json\")\n",
    "\n",
    "def run_bm25retriever_pipeline(\n",
    "    index_dir: str,\n",
    "    query_path: str,\n",
    "    output_dir: str,\n",
    "    top_k: int = 20,\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    storage_path = Path(index_dir)\n",
    "    if any(storage_path.glob(\"*.json\")):\n",
    "        index_dirs = [index_dir]\n",
    "    else :\n",
    "        index_dirs = sorted([d for d in Path(index_dir).iterdir() if d.is_dir()])\n",
    "    retrievers = [BM25Retriever.from_persist_dir(path=index_dir) for index_dir in index_dirs]\n",
    "    for retriever in retrievers:\n",
    "        retriever.similarity_top_k = top_k\n",
    "    print(\"加载索引完成...\")\n",
    "    \n",
    "    with open(query_path, 'r') as f:\n",
    "        query_data = json.load(f)\n",
    "        \n",
    "    results = []\n",
    "    for data in tqdm(query_data):\n",
    "        result = process_retrieval(data, retrievers)\n",
    "        results.append(result)\n",
    "        \n",
    "    split_and_save_results(results, output_dir)\n",
    "        \n",
    "def run_thread_retrieval_pipeline(\n",
    "    index_dir: str,\n",
    "    query_path: str,\n",
    "    output_dir: str,\n",
    "    model_type: str = \"dashscope\",\n",
    "    model_name: str = \"text-embedding-v2\",\n",
    "    api_key: str = None,\n",
    "    base_url: str = None,\n",
    "    use_rerank: bool = False,\n",
    "    rerank_model: str = \"gte-rerank-v2\",\n",
    "    max_workers = 16,\n",
    "    top_k: int = 20,\n",
    "):\n",
    "    \n",
    "    embed_model = UnifiedEmbedding(\n",
    "        model_type=model_type,\n",
    "        model_name=model_name,\n",
    "        api_key=api_key,\n",
    "        base_url=base_url\n",
    "    ).get_embed_model()\n",
    "    \n",
    "    Settings.embed_model = embed_model\n",
    "    \n",
    "    \"\"\"执行检索流程主函数\"\"\"\n",
    "    # 初始化组件\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    indexes = load_indexes_from_folder(index_dir)\n",
    "    retrievers = [index.as_retriever(similarity_top_k=top_k) for index in indexes]\n",
    "    print(\"加载索引完成...\")\n",
    "    \n",
    "    # 加载查询数据\n",
    "    with open(query_path, 'r') as f:\n",
    "        query_data = json.load(f)\n",
    "    \n",
    "    # 处理所有查询 使用线程池并行处理\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_retrieval, data, retrievers, use_rerank, rerank_model) for data in query_data]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(query_data)):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                results.append(result)\n",
    "    \n",
    "    split_and_save_results(results, output_dir)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T09:58:58.212487Z",
     "start_time": "2025-04-17T09:41:22.800442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run_thread_retrieval_pipeline(\n",
    "    index_dir=\"./embeddings/dashscope/balance_3/index_group_0\",\n",
    "    query_path=\"dataset/MultiHopRAG.json\",\n",
    "    output_dir=\"./rerank/dashscope/with_rerank_balance_3_0_30\",\n",
    "    use_rerank=True,\n",
    "    rerank_model=\"gte-rerank-v2\",\n",
    "    model_type=\"dashscope\",\n",
    "    model_name=\"text-embedding-v2\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    top_k=30,\n",
    "    max_workers=16,  # 根据机器性能调整\n",
    ")"
   ],
   "id": "23ec305aedc583fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载索引完成...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2556/2556 [16:36<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T07:09:31.890645Z",
     "start_time": "2025-04-17T06:22:32.398941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 同步\n",
    "run_thread_retrieval_pipeline(\n",
    "    index_dir=\"./embeddings/openai/balance_1\",\n",
    "    query_path=\"dataset/MultiHopRAG.json\",\n",
    "    output_dir=\"./rerank/openai/with_rerank_balance_1_20\",\n",
    "    use_rerank=False,\n",
    "    rerank_model=\"gte-rerank-v2\",\n",
    "    model_type=\"openai\",\n",
    "    model_name=\"text-embedding-3-large\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY_COST\"),\n",
    "    base_url=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    max_workers=8,\n",
    "    top_k=20,\n",
    ")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载索引完成...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2556/2556 [44:13<00:00,  1.04s/it] \n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T09:29:26.788619Z",
     "start_time": "2025-04-17T09:29:11.722668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "run_bm25retriever_pipeline(\n",
    "    index_dir=\"embeddings/bm25/balance_3/index_group_0\",\n",
    "    query_path=\"dataset/MultiHopRAG.json\",\n",
    "    output_dir=\"rerank/bm25/with_rerank_balance_3_0_30\",\n",
    "    top_k=30,\n",
    ")"
   ],
   "id": "c7698d5746039209",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载索引完成...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2556/2556 [00:10<00:00, 249.27it/s]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 4,
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def rerank_existing_file(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    rerank_model: str = \"gte-rerank-v2\",\n",
    "    max_workers: int = 64,\n",
    "):\n",
    "    \"\"\"\n",
    "    对已有检索结果进行重新排序\n",
    "    Args:\n",
    "        input_path: 输入文件路径（需包含retrieval_list字段）\n",
    "        output_path: 输出文件路径\n",
    "        rerank_model: 重排序模型名称\n",
    "        max_workers: 最大并发数\n",
    "    \"\"\"\n",
    "    # 读取原始文件\n",
    "    with open(input_path, 'r') as f:\n",
    "        original_data = json.load(f)\n",
    "\n",
    "    # 处理单个项目的重排序\n",
    "    def process_rerank(item: Dict) -> Dict:\n",
    "        \"\"\"执行重排序并更新结果\"\"\"\n",
    "        try:\n",
    "            nodes = item['retrieval_list']\n",
    "            if not nodes:\n",
    "                return item\n",
    "            \n",
    "            # 准备重排序参数\n",
    "            documents = [n['text'] for n in nodes]\n",
    "            \n",
    "            # 调用重排序API\n",
    "            rerank_result = TextReRank.call(\n",
    "                model=rerank_model,\n",
    "                query=item['query'],\n",
    "                documents=documents,\n",
    "                top_n=10,\n",
    "                return_documents=False\n",
    "            )\n",
    "            \n",
    "            # 更新排序结果\n",
    "            sorted_nodes = [nodes[r['index']] for r in rerank_result.output['results']]\n",
    "            \n",
    "            # 保留原始数据结构\n",
    "            return {\n",
    "                **item,\n",
    "                \"retrieval_list\": [\n",
    "                    {\n",
    "                        \"text\": node['text'],\n",
    "                        \"score\": node['score']  # 可保留原始分数或使用新分数\n",
    "                    } for node in sorted_nodes\n",
    "                ]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"处理失败: {item['query']} - {str(e)}\")\n",
    "            return item  # 返回原始数据\n",
    "\n",
    "    # 多线程处理\n",
    "    processed_data = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_rerank, item) for item in original_data]\n",
    "        \n",
    "        # 添加进度条和延迟控制\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), \n",
    "                          total=len(original_data),\n",
    "                          desc=\"Reranking\"):\n",
    "            processed_data.append(future.result())\n",
    "\n",
    "    # 保存结果\n",
    "    os.makedirs(Path(output_path).parent, exist_ok=True)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(processed_data, f, indent=2)"
   ],
   "id": "90143b3bd955aeb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rerank_existing_file(\n",
    "    input_path=\"rerank/dashscope/without_rerank_balance_2/retrieval_results.json\",\n",
    "    output_path=\"rerank/dashscope/with_rerank_balance_2/retrieval_results.json\",\n",
    "    rerank_model=\"gte-rerank-v2\",\n",
    "    max_workers=64,  # 建议根据API配额设置\n",
    ")"
   ],
   "id": "ff606ffc30c816f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
