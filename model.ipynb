{
 "cells": [
  {
   "cell_type": "code",
   "id": "8f9a5bc7d45b3c25",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-18T02:09:16.640525Z",
     "start_time": "2025-04-18T02:09:16.607840Z"
    }
   },
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "prefix = \"Below is a question followed by some context from different sources. Please answer the question based on the context. The answer to the question is a word or entity. If the provided information is insufficient to answer the question, respond 'Insufficient Information'. Answer directly without explanation.\"\n",
    "\n",
    "def process_qa_pipeline(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    model_name: str,\n",
    "    api_key: str,\n",
    "    api_url: str,\n",
    "    workers: int = 100,\n",
    "    \n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    完整的QA处理流水线\n",
    "    参数:\n",
    "    input_path: 输入文件路径（JSON格式）\n",
    "    output_path: 输出文件路径（JSON格式）\n",
    "    \n",
    "    返回:\n",
    "    处理结果列表，每个元素包含：\n",
    "        - query: 原问题\n",
    "        - prompt: 生成的完整提示\n",
    "        - model_answer: 模型回答\n",
    "        - gold_answer: 正确答案\n",
    "        - question_type: 问题类型\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r') as f:\n",
    "        process_data = json.load(f)\n",
    "\n",
    "    # 初始化OpenAI客户端\n",
    "    client = OpenAI(api_key=api_key, base_url=api_url)\n",
    "\n",
    "    def query_model(prompt: str) -> str:\n",
    "        \n",
    "        \"\"\"模型调用统一入口\"\"\"\n",
    "        if \"gpt\" in model_name.lower() or model_name == \"deepseek-reasoner\":\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model_name,  # 使用传入的模型名称参数\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"API调用失败: {str(e)}\")\n",
    "                return \"Error: API调用失败\"\n",
    "            \n",
    "        if model_name == \"qwq-plus\":      \n",
    "            answer_content = \"\"     # 定义完整回复\n",
    "            reasoning_content = \"\"\n",
    "            is_answering = False\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                stream=True,\n",
    "            )\n",
    "            for chunk in completion:\n",
    "                # 如果chunk.choices为空，则打印usage\n",
    "                if not chunk.choices:\n",
    "                    print(\"\\nUsage:\")\n",
    "                    print(chunk.usage)\n",
    "                else:\n",
    "                    delta = chunk.choices[0].delta\n",
    "                    # 打印思考过程\n",
    "                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content is not None:\n",
    "                        reasoning_content += delta.reasoning_content\n",
    "                    else:\n",
    "                        # 开始回复\n",
    "                        if delta.content != \"\" and is_answering is False:\n",
    "                            is_answering = True\n",
    "                        answer_content += delta.content\n",
    "            return answer_content\n",
    "\n",
    "    def process_single(item: Dict) -> Optional[Dict]:\n",
    "        \"\"\"处理单个数据项\"\"\"\n",
    "        try:\n",
    "            context = '--------------'.join(e['text'] for e in item['retrieval_list'])\n",
    "            prompt = f\"{prefix}\\n\\nQuestion:{item['query']}\\n\\nContext:\\n\\n{context}\"\n",
    "            \n",
    "            return {\n",
    "                'query': item['query'],\n",
    "                'prompt': prompt,\n",
    "                'model_answer': query_model(prompt),\n",
    "                'gold_answer': item['answer'],\n",
    "                'question_type': item['question_type']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"处理失败: {item.get('query','')} - {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    # 并行处理\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        futures = [executor.submit(process_single, item) for item in process_data]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(process_data)):\n",
    "            if (res := future.result()) is not None:\n",
    "                results.append(res)\n",
    "\n",
    "    # 结果保存逻辑\n",
    "    output_path = os.path.abspath(output_path)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "process_qa_pipeline(\n",
    "    input_path=\"rerank/dashscope/with_rerank_balance_1_20/retrieval_results_reranked.json\",\n",
    "    output_path=\"models/qwq_plus/balance_1_20.json\",\n",
    "    model_name=\"qwq-plus\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    api_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    workers=16,\n",
    ")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "process_qa_pipeline(\n",
    "    input_path=\"rerank/dashscope/with_rerank_balance_3_0_30/retrieval_results_reranked.json\",\n",
    "    output_path=\"models/gpt_4o_mini/balance_3_0_30.json\",\n",
    "    model_name=\"gpt-4o-mini-ca\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY_COST\"),\n",
    "    api_url=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    workers=16,\n",
    ")"
   ],
   "id": "c21c64947b32ba29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-04-17T23:03:51.766760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import time  \n",
    "from datetime import datetime, timedelta \n",
    "\n",
    "def schedule_task(target_hour, target_minute):\n",
    "    \"\"\"计算到目标时间的剩余秒数，并添加执行时间校验，白天跑太慢了，就算开多线程也会卡住，只能半夜跑，而且半夜api调用有优惠\"\"\"\n",
    "    now = datetime.now()\n",
    "    \n",
    "    # 方法1：直接构造完整时间对象\n",
    "    target_time = datetime(now.year, now.month, now.day, target_hour, target_minute)\n",
    "    \n",
    "    # 处理跨天\n",
    "    if target_time < now:\n",
    "        target_time += timedelta(days=1)\n",
    "\n",
    "    # 计算等待时间\n",
    "    delta = target_time - now\n",
    "    wait_seconds = delta.total_seconds()\n",
    "    \n",
    "    print(f\"当前时间: {now.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"计划执行时间: {target_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"等待 {wait_seconds:.2f} 秒 ({delta})\")\n",
    "\n",
    "    time.sleep(wait_seconds) \n",
    "\n",
    "    # 二次校验\n",
    "    current_after_sleep = datetime.now()\n",
    "    if current_after_sleep < target_time:\n",
    "        remaining = (target_time - current_after_sleep).total_seconds()\n",
    "        time.sleep(remaining)  # 再次正确调用\n",
    "    \n",
    "    print(f\"实际执行时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"程序入口\")\n",
    "    schedule_task(target_hour=1, target_minute=0)\n",
    "    \n",
    "    process_qa_pipeline(\n",
    "        input_path=\"rerank/dashscope/with_rerank_balance_3_0_30/retrieval_results_reranked.json\",\n",
    "        output_path=\"models/deepseek_r1/balance_3_0_30.json\",\n",
    "        model_name=\"deepseek-reasoner\",\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        api_url=\"https://api.deepseek.com\",\n",
    "        workers=100,\n",
    "    )\n",
    "        \n",
    "    \n",
    "    process_qa_pipeline(\n",
    "        input_path=\"rerank/dashscope/with_rerank_balance_1_20/retrieval_results_reranked.json\",\n",
    "        output_path=\"models/deepseek_r1/balance_1_20.json\",\n",
    "        model_name=\"deepseek-reasoner\",\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        api_url=\"https://api.deepseek.com\",\n",
    "        workers=100,\n",
    "    )\n",
    "    \n",
    "        \n",
    "    process_qa_pipeline(\n",
    "        input_path=\"rerank/dashscope/with_rerank_balance_3_30/retrieval_results_reranked.json\",\n",
    "        output_path=\"models/deepseek_r1/balance_3_30.json\",\n",
    "        model_name=\"deepseek-reasoner\",\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        api_url=\"https://api.deepseek.com\",\n",
    "        workers=100,\n",
    "    )\n",
    "    \n"
   ],
   "id": "450f5e3d968732cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "程序入口\n",
      "当前时间: 2025-04-18 07:03:51\n",
      "计划执行时间: 2025-04-19 01:00:00\n",
      "等待 64568.22 秒 (17:56:08.224139)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "prefix = \"Below is a question followed by some context from different sources. Please answer the question based on the context. The answer to the question is a word or entity. If the provided information is insufficient to answer the question, respond 'Insufficient Information'. Answer directly without explanation.\"\n",
    "\n",
    "def process_qa_pipeline(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    model_name: str,\n",
    "    api_key: str,\n",
    "    api_url: str,\n",
    "    workers: int = 100,\n",
    "    \n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    完整的QA处理流水线\n",
    "    参数:\n",
    "    input_path: 输入文件路径（JSON格式）\n",
    "    output_path: 输出文件路径（JSON格式）\n",
    "    \n",
    "    返回:\n",
    "    处理结果列表，每个元素包含：\n",
    "        - query: 原问题\n",
    "        - prompt: 生成的完整提示\n",
    "        - model_answer: 模型回答\n",
    "        - gold_answer: 正确答案\n",
    "        - question_type: 问题类型\n",
    "    \"\"\"\n",
    "    with open(input_path, 'r') as f:\n",
    "        process_data = json.load(f)\n",
    "\n",
    "    # 初始化OpenAI客户端\n",
    "    client = OpenAI(api_key=api_key, base_url=api_url)\n",
    "\n",
    "    def query_model(prompt: str) -> str:\n",
    "        \n",
    "        \"\"\"模型调用统一入口\"\"\"\n",
    "        if \"gpt\" in model_name.lower() or model_name == \"deepseek-reasoner\":\n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model_name,  # 使用传入的模型名称参数\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "            except Exception as e:\n",
    "                print(f\"API调用失败: {str(e)}\")\n",
    "                return \"Error: API调用失败\"\n",
    "            \n",
    "\n",
    "    def process_single(item: Dict) -> Optional[Dict]:\n",
    "        \"\"\"处理单个数据项\"\"\"\n",
    "        try:\n",
    "            context = '--------------'.join(e['text'] for e in item['retrieval_list'])\n",
    "            prompt = f\"{prefix}\\n\\nQuestion:{item['query']}\\n\\nContext:\\n\\n{context}\"\n",
    "            \n",
    "            return {\n",
    "                'query': item['query'],\n",
    "                'prompt': prompt,\n",
    "                'model_answer': query_model(prompt),\n",
    "                'gold_answer': item['answer'],\n",
    "                'question_type': item['question_type']\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"处理失败: {item.get('query','')} - {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    # 并行处理\n",
    "    results = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        futures = [executor.submit(process_single, item) for item in process_data]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(process_data)):\n",
    "            if (res := future.result()) is not None:\n",
    "                results.append(res)\n",
    "\n",
    "    # 结果保存逻辑\n",
    "    output_path = os.path.abspath(output_path)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(results, f)\n",
    "\n",
    "    return results"
   ],
   "id": "37262618f8964cdd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
